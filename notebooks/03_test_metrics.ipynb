{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oscar Nomination Prediction - Model Testing\n",
    "\n",
    "This notebook evaluates a trained transformer model on the test dataset to predict whether movie scripts were nominated for an Oscar.\n",
    "\n",
    "**Binary Classification Task:**\n",
    "- **0**: Not nominated for Oscar\n",
    "- **1**: Nominated for Oscar\n",
    "\n",
    "The notebook will:\n",
    "1. Load the pre-trained model and test data\n",
    "2. Make predictions on the test set\n",
    "3. Calculate accuracy metrics\n",
    "4. Visualize results with a confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies\n",
    "\n",
    "Import all necessary libraries for model loading, evaluation, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set the path to the trained model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../models/20260123_100epochs/transformer_best_ep7.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Class Definition\n",
    "\n",
    "The `OscarScriptDataset` class handles pre-tokenized movie scripts:\n",
    "- Pads or truncates sequences to a fixed length\n",
    "- Prepares data for batch processing\n",
    "- Returns token IDs and binary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OscarScriptDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for Oscar nomination prediction from movie scripts.\n",
    "\n",
    "    This dataset handles pre-tokenized movie scripts and their corresponding\n",
    "    Oscar nomination labels. It ensures all sequences are padded or truncated\n",
    "    to a fixed length for batch processing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenized_items, max_length=5000):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with pre-tokenized scripts.\n",
    "\n",
    "        Args:\n",
    "            tokenized_items: List of dicts containing:\n",
    "                - 'input_ids': List of token IDs representing the script\n",
    "                - 'target': Binary label (0=not nominated, 1=nominated)\n",
    "            max_length: Maximum sequence length for padding/truncation.\n",
    "                Longer sequences are truncated, shorter ones are padded with zeros.\n",
    "        \"\"\"\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Initialize lists to store processed inputs and targets\n",
    "        print(f\"Processing {len(tokenized_items)} pre-tokenized scripts...\")\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "\n",
    "        # Process each script in the dataset\n",
    "        for idx, item in enumerate(tokenized_items):\n",
    "            # Extract pre-tokenized token IDs\n",
    "            tokens = item['input_ids']\n",
    "\n",
    "            # Ensure uniform sequence length through truncation or padding\n",
    "            if len(tokens) > max_length:\n",
    "                # Truncate sequences that exceed max_length\n",
    "                tokens = tokens[:max_length]\n",
    "            else:\n",
    "                # Pad shorter sequences with zeros to reach max_length\n",
    "                # Note: 0 serves as the padding token ID\n",
    "                tokens = tokens + [0] * (max_length - len(tokens))\n",
    "\n",
    "            # Convert to PyTorch tensors and store\n",
    "            self.inputs.append(torch.tensor(tokens, dtype=torch.long))\n",
    "            self.targets.append(torch.tensor(item['target'], dtype=torch.long))  # Binary: 0 or 1\n",
    "\n",
    "            # Progress logging every 100 scripts\n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print(f\"  Processed {idx + 1}/{len(tokenized_items)} scripts\")\n",
    "\n",
    "        print(\"Processing complete!\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve a single sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx: Index of the sample to retrieve\n",
    "\n",
    "        Returns:\n",
    "            dict: Contains 'input_ids' (token tensor) and 'target' (label tensor)\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'input_ids': self.inputs[idx],\n",
    "            'target': self.targets[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture Definition\n",
    "\n",
    "The `OscarNomTransformer` uses a hierarchical transformer architecture:\n",
    "1. **Chunking**: Splits long scripts into manageable chunks\n",
    "2. **Encoder**: Processes each chunk independently\n",
    "3. **Aggregator**: Combines chunk representations to make final prediction\n",
    "\n",
    "This design allows the model to handle very long sequences (100k+ tokens) that wouldn't fit in standard transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OscarNomTransformer(nn.Module):\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.enc_d_model = config['enc_d_model']\n",
    "        self.enc_nhead = config['enc_nhead']\n",
    "        self.enc_dim_ff = config['enc_dim_ff']\n",
    "        \n",
    "        self.agg_d_model = config['agg_d_model']\n",
    "        self.agg_nhead = config['agg_nhead']\n",
    "        self.agg_dim_ff = config['agg_dim_ff']\n",
    "        \n",
    "        self.chunk_size = config['chunk_size']\n",
    "        self.enc_num_layers = config['enc_num_layers']\n",
    "        self.agg_num_layers = config['agg_num_layers']\n",
    "\n",
    "        self.token_emb = nn.Embedding(config['vocab_size'], config['enc_d_model'])\n",
    "        \n",
    "        self.enc_pos_enc = self._positional_encoder(config['chunk_size'], config['enc_d_model'])\n",
    "        self.agg_pos_enc = self._positional_encoder(config['max_seq_len'] // config['chunk_size'] + 1, config['agg_d_model'])\n",
    "        \n",
    "        # encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config['enc_d_model'],\n",
    "            nhead=config['enc_nhead'],\n",
    "            dim_feedforward=config['enc_dim_ff'],\n",
    "            dropout=config['dropout'],\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=config['enc_num_layers'])\n",
    "\n",
    "        if config['enc_d_model'] != config['agg_d_model']:\n",
    "            self.chunk_proj = nn.Linear(config['enc_d_model'], config['agg_d_model'])\n",
    "        else:\n",
    "            self.chunk_proj = nn.Identity()\n",
    "            \n",
    "        # aggregator\n",
    "        aggregator_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config['agg_d_model'],\n",
    "            nhead=config['agg_nhead'],\n",
    "            dim_feedforward=config['agg_dim_ff'],\n",
    "            dropout=config['dropout'],\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.aggregator = nn.TransformerEncoder(aggregator_layer, num_layers=config['agg_num_layers'])\n",
    "\n",
    "        self.dropout= nn.Dropout(config['dropout'])\n",
    "\n",
    "        self.classification_head = nn.Linear(config['agg_d_model'], 2)\n",
    "\n",
    "        # Initialize weights following GPT best practices\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        # Apply special scaled initialization for residual projections\n",
    "        self._init_residual_projections()\n",
    "\n",
    "    def _positional_encoder(self, max_seq_len, d_model):\n",
    "        position = torch.arange(max_seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"\n",
    "        Initialize weights following GPT best practices.\n",
    "        - Embeddings: N(0, 0.02)\n",
    "        - Linear layers: N(0, 0.02) for weights, 0 for biases\n",
    "        - LayerNorm: standard initialization (weight=1, bias=0)\n",
    "        \"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "    def _init_residual_projections(self):\n",
    "        \"\"\"\n",
    "        Apply scaled initialization to residual projection layers.\n",
    "        Following GPT-2, scale by 1/sqrt(2*num_layers) for stability in deep networks.\n",
    "        \"\"\"\n",
    "        total_layers = self.enc_num_layers + self.agg_num_layers\n",
    "\n",
    "        # Scale residual projections in encoder layers\n",
    "        for layer in self.encoder.layers:\n",
    "            # Scale the second linear layer in the feedforward network (residual projection)\n",
    "            torch.nn.init.normal_(layer.linear2.weight, mean=0.0, std=0.02/math.sqrt(2 * total_layers))\n",
    "            # Scale the output projection in multi-head attention\n",
    "            torch.nn.init.normal_(layer.self_attn.out_proj.weight, mean=0.0, std=0.02/math.sqrt(2 * total_layers))\n",
    "\n",
    "        # Scale residual projections in aggregator layers\n",
    "        for layer in self.aggregator.layers:\n",
    "            # Scale the second linear layer in the feedforward network (residual projection)\n",
    "            torch.nn.init.normal_(layer.linear2.weight, mean=0.0, std=0.02/math.sqrt(2 * total_layers))\n",
    "            # Scale the output projection in multi-head attention\n",
    "            torch.nn.init.normal_(layer.self_attn.out_proj.weight, mean=0.0, std=0.02/math.sqrt(2 * total_layers))\n",
    "\n",
    "    def forward(self, src):\n",
    "        batch_size, seq_len = src.shape\n",
    "        \n",
    "        # 1. Chunk the input\n",
    "        # Pad seq_len to be divisible by chunk_size\n",
    "        remainder = seq_len % self.chunk_size\n",
    "        if remainder != 0:\n",
    "            pad_len = self.chunk_size - remainder\n",
    "            src = F.pad(src, (0, pad_len), value = 0) # don't pad on left, pad on right with pad_len zeros\n",
    "            seq_len = src.shape[1] # now reference updated/padded sequence length\n",
    "\n",
    "        num_chunks = seq_len // self.chunk_size\n",
    "\n",
    "        # Then reshape to (batch_size * num_chunks, chunk_size)\n",
    "        src = src.view((batch_size, num_chunks, self.chunk_size))\n",
    "        src = src.view((batch_size * num_chunks, self.chunk_size))\n",
    "\n",
    "        # 2. Embed tokens, add positional encoding\n",
    "        # Shape: (batch_size * num_chunks, chunk_size, enc_d_model)\n",
    "        src_emb = self.token_emb(src) * math.sqrt(self.enc_d_model)\n",
    "        src_emb += self.enc_pos_enc[:, :self.chunk_size].to(src_emb.device)\n",
    "        src_emb = self.dropout(src_emb)\n",
    "        \n",
    "        # 3. Encode all chunks (can process in parallel or loop)\n",
    "        # Shape after encoder: (batch_size * num_chunks, chunk_size, enc_d_model)\n",
    "        enc_chunks = self.encoder(src_emb)\n",
    "        \n",
    "        # 4. Pool each chunk to single vector (mean pool over token dimension)\n",
    "        # Shape: (batch_size * num_chunks, enc_d_model)\n",
    "        chunk_embs = enc_chunks.mean(dim=1)\n",
    "\n",
    "        # 5. Reshape back to (batch_size, num_chunks, enc_d_model)\n",
    "        chunk_embs = chunk_embs.view((batch_size, num_chunks, self.enc_d_model))\n",
    "\n",
    "        # 6. Project if needed, add chunk positional encoding\n",
    "        # Shape: (batch_size, num_chunks, agg_d_model)\n",
    "        chunk_embs = self.chunk_proj(chunk_embs) * math.sqrt(self.agg_d_model)\n",
    "        chunk_embs += self.agg_pos_enc[:, :num_chunks, :].to(chunk_embs.device)\n",
    "        chunk_embs = self.dropout(chunk_embs)\n",
    "        \n",
    "        # 7. Run through aggregator\n",
    "        # Shape: (batch_size, num_chunks, agg_d_model)\n",
    "        agg_out = self.aggregator(chunk_embs)\n",
    "        \n",
    "        # 8. Pool to single vector (mean pool over chunk dimension)\n",
    "        # Shape: (batch_size, agg_d_model)\n",
    "        agg_out = agg_out.mean(dim=1)\n",
    "        \n",
    "        # 9. Classification head\n",
    "        # Shape: (batch_size, 2)\n",
    "        logits = self.classification_head(agg_out)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Set Random Seed for Reproducibility\n",
    "\n",
    "Ensure consistent results across multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "torch.cuda.manual_seed_all(1337)  # For multi-GPU setups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Hyperparameters\n",
    "\n",
    "Define the configuration that matches the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # Chunking parameters\n",
    "    'chunk_size': 512,              # Size of each chunk for hierarchical processing\n",
    "    'vocab_size': 50257,             # Vocabulary size (GPT-2 tokenizer)\n",
    "\n",
    "    # Encoder transformer (processes individual chunks)\n",
    "    'enc_d_model': 128,              # Embedding dimension for encoder\n",
    "    'enc_nhead': 4,                  # Number of attention heads in encoder\n",
    "    'enc_dim_ff': 512,               # Feedforward dimension in encoder\n",
    "    'enc_num_layers': 2,             # Number of encoder transformer layers\n",
    "\n",
    "    # Aggregator transformer (combines chunk representations)\n",
    "    'agg_d_model': 128,              # Embedding dimension for aggregator\n",
    "    'agg_nhead': 4,                  # Number of attention heads in aggregator\n",
    "    'agg_dim_ff': 512,               # Feedforward dimension in aggregator\n",
    "    'agg_num_layers': 2,             # Number of aggregator transformer layers\n",
    "\n",
    "    # Sequence parameters\n",
    "    'max_seq_len': 106578,           # Maximum sequence length (full script)\n",
    "\n",
    "    # Regularization\n",
    "    'dropout': 0.3,                  # Dropout probability for all layers\n",
    "\n",
    "    # Training hyperparameters\n",
    "    'batch_size': 1,                 # Number of samples per batch\n",
    "    'peak_lr': 1e-4,                 # Peak learning rate (reached after warmup)\n",
    "    'weight_decay': 0.05             # L2 regularization coefficient for AdamW\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load Test Data\n",
    "\n",
    "Load the pre-tokenized test dataset from pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_token_path = '../src/token_data/test_tokenized.pkl'\n",
    "\n",
    "# Load pre-tokenized test data\n",
    "# Data is expected to be a list of dicts with 'input_ids' and 'target' keys\n",
    "print(f\"Loading test data from {test_token_path}...\")\n",
    "with open(test_token_path, 'rb') as f:\n",
    "    test_tokenized_items = pickle.load(f)\n",
    "print(f\"Loaded {len(test_tokenized_items)} test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Device Setup\n",
    "\n",
    "Use GPU if available, otherwise fall back to CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create Dataset and DataLoader\n",
    "\n",
    "Wrap the tokenized data in PyTorch Dataset and DataLoader for batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch dataset\n",
    "print(\"\\nCreating PyTorch dataset...\")\n",
    "test_dataset = OscarScriptDataset(test_tokenized_items, max_length=config['max_seq_len'])\n",
    "\n",
    "# Create dataloader for batch processing\n",
    "print(\"\\nCreating DataLoader...\")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,                   # No shuffling needed for test data\n",
    "    num_workers=0                    # Single-threaded loading (set >0 for parallel)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Initialize and Load Model\n",
    "\n",
    "Create the model architecture and load the pre-trained weights from the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "print(\"\\nInitializing model...\")\n",
    "model = OscarNomTransformer(config).to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Load pre-trained weights from checkpoint\n",
    "print(f\"\\nLoading model weights from {model_path}...\")\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Model weights loaded successfully from epoch {checkpoint['epoch']}!\")\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Evaluate Model on Test Set\n",
    "\n",
    "Run inference on all test samples and collect predictions for accuracy calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluating model on test set...\")\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "all_probabilities = []  # Store probabilities for ROC curve\n",
    "\n",
    "# Disable gradient computation for inference\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(test_dataloader):\n",
    "        # Move data to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        targets = batch['target'].to(device)\n",
    "\n",
    "        # Forward pass - get model predictions\n",
    "        logits = model(input_ids)\n",
    "\n",
    "        # Get predicted class (0 or 1)\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "        # Get probabilities for positive class (nominated = 1)\n",
    "        probabilities = F.softmax(logits, dim=1)[:, 1]  # Probability of class 1\n",
    "\n",
    "        # Update accuracy metrics\n",
    "        correct += (predictions == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "        # Store predictions, targets, and probabilities\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "        all_probabilities.extend(probabilities.cpu().numpy())\n",
    "\n",
    "        # Progress update\n",
    "        if (batch_idx + 1) % 5 == 0:\n",
    "            print(f\"  Processed {batch_idx + 1}/{len(test_dataloader)} batches\")\n",
    "\n",
    "# Calculate and display final accuracy\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Test Accuracy: {accuracy:.2f}% ({correct}/{total} correct)\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Generate Confusion Matrix\n",
    "\n",
    "Visualize the model's performance by showing:\n",
    "- **True Negatives**: Correctly predicted not nominated\n",
    "- **False Positives**: Incorrectly predicted nominated\n",
    "- **False Negatives**: Incorrectly predicted not nominated\n",
    "- **True Positives**: Correctly predicted nominated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGenerating confusion matrix...\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(all_targets, all_predictions)\n",
    "\n",
    "# Display confusion matrix as text\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(f\"{'':20} Predicted Not Nominated  Predicted Nominated\")\n",
    "print(f\"{'Actually Not Nominated':20} {cm[0, 0]:>22} {cm[0, 1]:>19}\")\n",
    "print(f\"{'Actually Nominated':20} {cm[1, 0]:>22} {cm[1, 1]:>19}\")\n",
    "\n",
    "# Visualize confusion matrix\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm,\n",
    "    display_labels=['Not Nominated (0)', 'Nominated (1)']\n",
    ")\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.title('Confusion Matrix - Oscar Nomination Predictions')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "output_path = '../models/20260123_100epochs/confusion_matrix.png'\n",
    "plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"\\nConfusion matrix saved to {output_path}\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Classification Metrics Summary\n",
    "\n",
    "Comprehensive evaluation metrics for binary classification:\n",
    "\n",
    "- **Accuracy**: Overall percentage of correct predictions (both classes)\n",
    "- **Precision**: Of all predicted nominations, what percentage were actually nominated? (TP / (TP + FP))\n",
    "- **Recall**: Of all actual nominations, what percentage did we correctly identify? (TP / (TP + FN))\n",
    "- **F1 Score**: Harmonic mean of precision and recall, balancing both metrics\n",
    "- **Macro-F1**: Average of F1 scores for both classes, treating them equally regardless of class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. ROC Curve and AUC\n",
    "\n",
    "The **ROC (Receiver Operating Characteristic) curve** plots the trade-off between:\n",
    "- **True Positive Rate (TPR)**: Sensitivity - how many actual positives are correctly identified\n",
    "- **False Positive Rate (FPR)**: How many actual negatives are incorrectly classified as positive\n",
    "\n",
    "The **AUC (Area Under the Curve)** summarizes the ROC curve into a single metric:\n",
    "- **AUC = 1.0**: Perfect classifier\n",
    "- **AUC = 0.5**: Random classifier (no better than chance)\n",
    "- **AUC < 0.5**: Worse than random (predictions are inverted)\n",
    "\n",
    "A higher AUC indicates better model performance at distinguishing between nominated and non-nominated scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nComputing ROC curve and AUC...\")\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(all_targets, all_probabilities)\n",
    "\n",
    "# Compute Area Under the Curve (AUC)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "print(f\"AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random classifier (AUC = 0.50)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curve - Oscar Nomination Predictions', fontsize=14)\n",
    "plt.legend(loc=\"lower right\", fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "roc_output_path = '../models/20260123_100epochs/roc_curve.png'\n",
    "plt.savefig(roc_output_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"ROC curve saved to {roc_output_path}\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Classification Metrics Summary\n",
    "\n",
    "Comprehensive evaluation metrics for binary classification:\n",
    "\n",
    "- **Accuracy**: Overall percentage of correct predictions (both classes)\n",
    "- **Precision**: Of all predicted nominations, what percentage were actually nominated? (TP / (TP + FP))\n",
    "- **Recall**: Of all actual nominations, what percentage did we correctly identify? (TP / (TP + FN))\n",
    "- **F1 Score**: Harmonic mean of precision and recall, balancing both metrics\n",
    "- **Macro-F1**: Average of F1 scores for both classes, treating them equally regardless of class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nComputing classification metrics...\")\n",
    "\n",
    "# Calculate metrics\n",
    "acc = accuracy_score(all_targets, all_predictions)\n",
    "precision = precision_score(all_targets, all_predictions, average='binary', pos_label=1)\n",
    "recall = recall_score(all_targets, all_predictions, average='binary', pos_label=1)\n",
    "f1 = f1_score(all_targets, all_predictions, average='binary', pos_label=1)\n",
    "macro_f1 = f1_score(all_targets, all_predictions, average='macro')\n",
    "\n",
    "# Create DataFrame\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'Macro-F1'],\n",
    "    'Value': [acc, precision, recall, f1, macro_f1]\n",
    "})\n",
    "\n",
    "# Format values as percentages\n",
    "metrics_df['Value (%)'] = metrics_df['Value'].apply(lambda x: f'{x * 100:.2f}%')\n",
    "\n",
    "# Display the table\n",
    "print(\"\\nClassification Metrics:\")\n",
    "print(\"=\" * 50)\n",
    "display(metrics_df[['Metric', 'Value (%)']])\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
